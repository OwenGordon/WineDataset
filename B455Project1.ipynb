{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GHA7dvihWUI"
   },
   "source": [
    "# B455 Project 1 - Designing a Multi-layer Perceptron\n",
    "#### By Owen Gordon\n",
    "\n",
    "The goal of this project was to design a multi layer perceptron to classify types of wine. The attributes for the given inputs were: 1) Alcohol, 2) Malic acid, 3) Ash, 4) Alcalinity of ash, 5) Magnesium, 6) Total phenols, 7) Flavanoids, 8) Nonflavanoid phenols, 9) Proanthocyanins, 10) Color intensity, 11) Hue, 12) OD280/OD315 of diluted wines, and 13) Proline.<br><br>\n",
    "\n",
    "To begin the data must be read in, and then the data needs to be separated into the input and the targets. I used a Pandas dataframe to read the data and I used numpy arrays to store the data.<br><br>\n",
    "\n",
    "Note: reading the csv assumes the file containing the input data is called wine.csv and is contained in the same directory as this .ipynb file. If this is not the case, the path to the file inside pd.read_csv must be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pfe_QtGhhVmy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "df = pd.read_csv(\"./wine.csv\", header=None) \n",
    "INPUTS = df[df.columns[1:14]].to_numpy()\n",
    "TARGETS = df[df.columns[:1]].to_numpy().flatten()\n",
    "random_state = np.random.get_state()\n",
    "np.random.shuffle(INPUTS)\n",
    "np.random.set_state(random_state)\n",
    "np.random.shuffle(TARGETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay1kBBMjaSKl"
   },
   "source": [
    "Then, since all the data points in the input data is continuous, the data should be normalized. I used the traditional feature scaling technique using the equation: $z = \\frac{x_{i} - \\mu}{\\sigma}$ where $x_i$ is unnormalized input value for the i-th element, $\\mu$ is the mean of this given feature, $\\sigma$ is the standard deviation of this feature, and $z$ is the normalized input value.<br><br>\n",
    "\n",
    "This was easily completed since the inputs are all stored as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pKYn5dvfcqzc"
   },
   "outputs": [],
   "source": [
    "  def normalize_data(inputs):\n",
    "    return (inputs - inputs.mean(axis=0)) / (inputs.std(axis=0))\n",
    "\n",
    "  NORMALIZED_INPUTS = normalize_data(INPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plGG0RrtpW4L"
   },
   "source": [
    "Now that the data is prepped, the next step can be to build the classifiers. The first type of classifier that is going to be used is the baseline classifier. This classifier is uninformed and will be making a random guess at which class the wine belongs to. Given that this is the baseline randome estimate, the MLP should predict with much greater accuracy the classes of wine.\n",
    "\n",
    "But first, the baseline classifyer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79ZPoJEssTxT",
    "outputId": "cb059740-eb0c-4e5f-b0a1-16fb010e8602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 29.21%\n"
     ]
    }
   ],
   "source": [
    "def baseline_prediction(input):\n",
    "  attribute = input[np.random.randint(13)]\n",
    "  return int(attribute % 3) + 1\n",
    "\n",
    "def perform_baseline_prediction(inputs, targets):\n",
    "  predictions = []\n",
    "\n",
    "  for input in inputs:\n",
    "    predictions.append(baseline_prediction(input))\n",
    "\n",
    "  predictions = np.array(predictions)\n",
    "  return sum(predictions == targets) / len(targets)\n",
    "\n",
    "baseline_accuracy = perform_baseline_prediction(INPUTS, TARGETS)\n",
    "print('Baseline Accuracy: ' + str(int(baseline_accuracy * 10000) / 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwkfexqxetcp"
   },
   "source": [
    "The baseline accuracy should be around 33%, or accurate 1/3 of the time. This is becuase there are 3 possible classes, so a random guess has a 1 in 3 chance of being the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzTVWMMXsm-_"
   },
   "source": [
    "Next is the Multi Layer Perceptron classifier. This classifer will be an object that contains the weights for the network as well as a specified learning size and the neural network architecture. When a new MLP is initialized, the weight values are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6h_Q-hlgzOTy"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "  def __init__(self, neural_net_structrue, step_size=0.1):\n",
    "    self.step_size = step_size\n",
    "    self.network_structure = neural_net_structrue\n",
    "    self.layers = len(self.network_structure)\n",
    "    self.input_layer = self.network_structure[0]\n",
    "    self.output_layer = self.network_structure[-1]\n",
    "    self.network_params = self.initialize()\n",
    "\n",
    "  def initialize(self):\n",
    "    params = {}\n",
    "    for layer in range(1, self.layers):\n",
    "      params['Weights' + str(layer)] = np.random.randn(\n",
    "          self.network_structure[layer-1],\n",
    "          self.network_structure[layer]) * 0.1\n",
    "      params['Bias' + str(layer)] = np.zeros(self.network_structure[layer])\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWU9SwvDzwtD"
   },
   "source": [
    "Just a few more steps before 5-fold Cross validation can be performed. The first is implementing feedforward algorithm which will be used to calculate the activations of each layer in the network.\n",
    "\n",
    "The following formula was used for the feedforward algorithm: $h_\\zeta=\\sum_{i=1}^N x_iw_i + b$ <br>\n",
    "$N$ is the number of nodes in the layer<br>$x_i$ is the _ith_ input value<br>$w_i$ is the _ith_ weight value <br>$b$ is the bias for this layer<br>$h_\\zeta$ is the activation value for this layer.<br><br>\n",
    "Then the hidden layer activation function is given $h_\\zeta$ to calculate the activation value for this layer. The sigmoid activation fuction is the function used. This formula is: $a_\\zeta = g(h_\\zeta) = \\frac{1}{1 + e^{h_\\zeta}}$\n",
    "<br><br>\n",
    "For the output layer, a softmax activation function was used because there are 3 classes for the output. The softmax formula for the activation of the *kth* node, $h_k$, is: $y_k = g(h_k) = \\frac{e^{h_k}}{\\sum_{j = 1}^N e^{h_j}}$ <br>\n",
    "Where $\\sum_{j = 1}^N e^{h_j}$ is the sum of activations $h_j$ for the $N$ output nodes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aBjDNZFrpCc3"
   },
   "outputs": [],
   "source": [
    "def feed_forward(MLP, inputs):\n",
    "  activations = [inputs]\n",
    "  for layer in range(1, MLP.layers):\n",
    "    activation = np.dot(activations[layer - 1], MLP.network_params['Weights' + str(layer)]) + MLP.network_params['Bias' + str(layer)]\n",
    "\n",
    "    if layer < MLP.layers:\n",
    "      activation = 1 / (1 + np.exp(-activation))\n",
    "   \n",
    "    if layer == MLP.layers:\n",
    "      activation = np.exp(activation) / np.sum(np.exp(activation))\n",
    "\n",
    "    activations.append(activation)\n",
    "  return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePmQ4-AS0Kbf"
   },
   "source": [
    "The next step is implementing the back propogation algorithm which will update the layer weights.<br><br>\n",
    "\n",
    "Starting with the output layer, the error of the output layer is calculated using: $\\delta_o(k) = (y_k - t_k)y_k(1 - y_k)$<br> \n",
    "Where $k$ is the _kth_ output node<br>\n",
    "$y_k$ is the activation value of the _kth_ output node<br>\n",
    "$t_k$ is the target value of the _kth_ output node\n",
    "<br><br>\n",
    "\n",
    "The error of the hidden layers is calculated slightly differently using: $\\delta_h(\\zeta) = a_\\zeta(1 - a_\\zeta)\\sum_{k=1}^Nw_\\zeta\\delta_o(k)$ <br> Where $\\zeta$ is the *$\\zeta$th* node in the hidden layer<br>\n",
    "$a_\\zeta$ is the activation of the *$\\zeta$th* node in the hidden layer<br>\n",
    "$w_\\zeta$ is the weight of the *$\\zeta$th* node in the hidden layer<br>\n",
    "$N$ is the number of nodes in the output layer (or the layer 1 layer forward)<br>\n",
    "$\\delta_o(k)$ is the error of the _kth_ node in the layer one layer forward\n",
    "<br><br>\n",
    "\n",
    "The weights and biases (bias is $w_0$) in each layer are then updated according to: $w_k = w_k - \\eta\\delta_o(k)h_\\zeta$<br>\n",
    "Where $w_k$ is _kth_ weight of the current layer<br>\n",
    "$\\eta$ is the network learning rate<br>\n",
    "$\\delta_o$ is the error of the layer one layer forward layer<br>\n",
    "$h_\\zeta$ is the activation of the previous layer<br><br>\n",
    "\n",
    "$\\delta_o(k)h_\\zeta$ is called the _layer gradient_ in my program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qesqBVBi0H3h"
   },
   "outputs": [],
   "source": [
    "def back_propogation(MLP, targets, activations):\n",
    "  current_layer = MLP.layers - 1\n",
    "  output_layer_delta = activations[current_layer] - targets\n",
    "  layer_gradient = np.dot(np.transpose([activations[current_layer - 1]]), output_layer_delta)\n",
    "  bias_gradient = output_layer_delta.mean(axis=0)\n",
    "  MLP.network_params['Weights' + str(current_layer)] = MLP.network_params['Weights' + str(current_layer)] - (MLP.step_size * layer_gradient)\n",
    "  MLP.network_params['Bias' + str(current_layer)] = MLP.network_params['Bias' + str(current_layer)] - (MLP.step_size * bias_gradient)\n",
    "  layer_delta = output_layer_delta\n",
    "\n",
    "  current_layer -= 1\n",
    "  while current_layer > 0:\n",
    "    previous_delta = layer_delta\n",
    "    layer_delta = np.dot(previous_delta, np.transpose(MLP.network_params['Weights' + str(current_layer + 1)])) * activations[current_layer] * (1 - activations[current_layer])\n",
    "    layer_gradient = np.dot(np.transpose([activations[current_layer - 1]]), layer_delta)\n",
    "    bias_gradient = layer_gradient.mean(axis=0)\n",
    "    MLP.network_params['Weights' + str(current_layer)] = MLP.network_params['Weights' + str(current_layer)] - (MLP.step_size * layer_gradient)\n",
    "    MLP.network_params['Bias' + str(current_layer)] = MLP.network_params['Bias' + str(current_layer)] - (MLP.step_size * bias_gradient)\n",
    "    current_layer -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QQMVPK--JLl"
   },
   "source": [
    "The last step before the 5-fold algorithm is implementing the training and testing algorithms. <br><br>The training algorithm sends the training data through the feedforward algorithm, then compares the feedforward activations to the targets, and updates the weights of the network accordingly using the backpropogation algorithm. The number of iterations of this back and forth is the _epochs_ the network experiences. A higher epoch value means a longer time for a network to learn, but a higher chance for overfitting.\n",
    "<br><br>\n",
    "The testing algorithm is very similar to the training algorithm, except instead of sending the feedforward activations through the backpropogation algorithm, the activation values are compared to the targets, and the number of correct activations is the accuracy for this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x2nvyK5F0BXR"
   },
   "outputs": [],
   "source": [
    "def train(MLP, training_inputs, training_targets, epochs):\n",
    "  inputs = training_inputs\n",
    "  targets = np.zeros((len(training_targets), 3))\n",
    "  for i in range(len(training_targets)):\n",
    "    t = training_targets[i]\n",
    "    targets[i][t-1] = 1\n",
    "\n",
    "  for i in range(epochs):\n",
    "    np.random.rand()\n",
    "    random_state = np.random.get_state()\n",
    "    np.random.shuffle(inputs)\n",
    "    np.random.set_state(random_state)\n",
    "    np.random.shuffle(targets)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "      activations = feed_forward(MLP, inputs[i])\n",
    "      back_propogation(MLP, [targets[i]], activations)\n",
    "\n",
    "def test(MLP, testing_inputs, testing_targets):\n",
    "\n",
    "  activations = feed_forward(MLP, testing_inputs)\n",
    "  predictions = activations[-1]\n",
    "\n",
    "  p = []\n",
    "  for i in range(len(predictions)):\n",
    "    p.append(predictions[i].argmax() + 1)\n",
    "\n",
    "  predictions = np.array(p)\n",
    "  return sum(predictions == testing_targets) / len(testing_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aomEHmXNc8t_"
   },
   "source": [
    "The data is now ready to go into a 5-fold cross-validation method and be prepped to begin the MLP training. The inputs and targets will both be passed into the 5-fold method, and the first step from there will be to split the data into 5 partitions. Then the data will enter a loop and iterate over the 5 classes, giving each class one chance to act as the testing sample as the others act as the training samples. The accuracy of this testing sample will then be recorded, and the mean of these accuracies is the network accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kDhxgnLWzYXe"
   },
   "outputs": [],
   "source": [
    "def five_fold_cross_val(neural_net_structrue, inputs, targets, epochs=200):\n",
    "  def five_fold(inputs, targets):\n",
    "    D_inputs = []\n",
    "    D_targets = []\n",
    "    D_dict = {'d1': [], 'd2': [], 'd3': [], 'd4': [], 'd5': []}\n",
    "    D_target_dict = {'d1': [], 'd2': [], 'd3': [], 'd4': [], 'd5': []}\n",
    "\n",
    "    for x in range(len(inputs)):\n",
    "      D_dict['d' + str((x % 5) + 1)].append(inputs[x])\n",
    "      D_target_dict['d' + str((x % 5) + 1)].append(targets[x])\n",
    "    for k in D_dict.keys():\n",
    "      D_inputs.append(np.array(D_dict[k]))\n",
    "      D_targets.append(np.array(D_target_dict[k]))\n",
    "    return D_inputs, D_targets\n",
    "\n",
    "  D, t = five_fold(inputs, targets)\n",
    "\n",
    "  accuracies = []\n",
    "  for i in range(len(D)):\n",
    "    testing_inputs = D[i]\n",
    "    training_inputs = []\n",
    "    testing_targets = t[i]\n",
    "    training_targets = []\n",
    "    for j in range(len(D)):\n",
    "      if i != j:\n",
    "        training_inputs.extend(D[j])\n",
    "        training_targets.extend(t[j])\n",
    "    training_inputs = np.array(training_inputs)\n",
    "    training_targets = np.array(training_targets)\n",
    "\n",
    "    mlp = MultiLayerPerceptron(neural_net_structrue)\n",
    "    train(mlp, training_inputs, training_targets, epochs)\n",
    "    accuracy = test(mlp, testing_inputs, testing_targets)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "  return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbBKzC0_ofy"
   },
   "source": [
    "The only other necessary step is to create the network architecture for the neural network. This means specifiying the number of nodes in each layer. This information is contained in a list where the first element of the list is the number of nodes in the input layer, the last is the number of nodes in the output layer, and any other elements are the number of nodes in each hidden layer.<br><br>\n",
    "\n",
    "The inputs, targets, and network architecture are sent into the 5-fold cross validation method, and the network is trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQdgNLXH_v9m",
    "outputId": "9567ca02-f125-4669-a533-480b5c38095b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 98.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "basic_nn_structure = [13, 6, 3]\n",
    "\n",
    "mlp_accuracy = five_fold_cross_val(basic_nn_structure, NORMALIZED_INPUTS, TARGETS)\n",
    "\n",
    "print('MLP Accuracy: ' + str(mlp_accuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URbKl3P6hgsx"
   },
   "source": [
    "Due to random initialization and random shuffling of the inputs, the final accuracy of the network varies slightly, but using a basic neural network, the accuracy is generally around 97% - 98%. Even 97% accuracy beats the baseline classifyer, so the network appears to be learning, and it also appears that the data is seperable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYPC3sD-T3mI"
   },
   "source": [
    "There are a few things that can be modified to find the optimal network architecture. First, the number of hidden layers can be adjusted. Second, the number of nodes in the hidden layers can be adjusted. <br><br>\n",
    "\n",
    "By experimenting with these two settings, an optimal network can hopefully be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMGSmyj-tT5O"
   },
   "source": [
    "There are many possible factors when choosing network paramters. One set up that is common is to choose a number of nodes between the number of output nodes and input nodes. Another common idea is to choose a number of nodes far greater than the number of input nodes. I am going to try a variety of layer and node structures to see which architecture performs the best.<br><br>\n",
    "\n",
    "Due to computational limits, network architectures with many layers, or layers with large numbers of neurons take too long to analyze. I am going to only look at smaller networks, with maybe a few exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVNnOFv2zt_S",
    "outputId": "ec6af2ad-c5ce-4db0-f9f5-9213fdcb154e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tests on network structures with number of hidden nodes in between number of input nodes and output nodes\n",
      "Network testing complete\n",
      "\n",
      "Moving onto testing networks with a single large hidden layer\n",
      "Network testing complete\n",
      "\n",
      "Moving onto testing networks with many hidden layers\n",
      "Network testing complete\n",
      "\n",
      "Moving onto testing networks with both large hidden layers, and many hidden layers\n",
      "Network testing complete\n",
      "\n",
      "Results:\n",
      "In between results:\n",
      "\tnodes = 10: 98.31746031746033%\n",
      "\tnodes = 7: 97.76190476190474%\n",
      "\tnodes = 4: 98.33333333333331%\n",
      "Large hidden number of nodes in hidden layer results:\n",
      "\tnodes = 50: 98.31746031746033%\n",
      "\tnodes = 500: 98.31746031746033%\n",
      "\tnodes = 1000: 98.31746031746033%\n",
      "Many hidden layers results:\n",
      "\tlayers = 5, nodes = 4: 29.841269841269842%\n",
      "\tlayers = 7, nodes = 7: 34.85714285714286%\n",
      "Multiple hidden layers and many nodes results:\n",
      "\tlayer_1 = 50 nodes, layer_2 = 25 nodes, layer_3 = 7 nodes: 39.333333333333336%\n",
      "\tlayer_1 = 7 nodes, layer_2 = 25 nodes, layer_3 = 50 nodes: 34.285714285714285%\n"
     ]
    }
   ],
   "source": [
    "in_between_structure_1 = [13, 10, 3]\n",
    "in_between_structure_2 = [13, 7, 3]\n",
    "in_between_structure_3 = [13, 4, 3]\n",
    "\n",
    "large_hidden_layer = [13, 50, 3]\n",
    "extra_large_hidden_layer = [13, 500, 3]\n",
    "extremely_large_hidden_layer = [13, 1000, 3]\n",
    "\n",
    "many_smaller_hidden_layers = [13, 4, 4, 4, 4, 4, 3]\n",
    "many_larger_hidden_layers = [13, 7, 7, 7, 7, 7, 3]\n",
    "\n",
    "large_to_small_hidden = [13, 50, 25, 7, 3]\n",
    "small_to_large_hidden = [13, 7, 25, 50, 3]\n",
    "\n",
    "print(\"Beginning tests on network structures with number of hidden nodes in between number of input nodes and output nodes\")\n",
    "in_between_structure_1_accruacy = five_fold_cross_val(in_between_structure_1, NORMALIZED_INPUTS, TARGETS)\n",
    "in_between_structure_2_accruacy = five_fold_cross_val(in_between_structure_2, NORMALIZED_INPUTS, TARGETS)\n",
    "in_between_structure_3_accruacy = five_fold_cross_val(in_between_structure_3, NORMALIZED_INPUTS, TARGETS)\n",
    "print(\"Network testing complete\\n\\nMoving onto testing networks with a single large hidden layer\")\n",
    "\n",
    "large_hidden_layer_accuracy = five_fold_cross_val(large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "extra_large_hidden_layer_accuracy = five_fold_cross_val(extra_large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "extremely_large_hidden_layer_accuracy = five_fold_cross_val(extremely_large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "print(\"Network testing complete\\n\\nMoving onto testing networks with many hidden layers\")\n",
    "\n",
    "many_smaller_hidden_layers_accuracy = five_fold_cross_val(many_smaller_hidden_layers, NORMALIZED_INPUTS, TARGETS)\n",
    "many_larger_hidden_layers_accuracy = five_fold_cross_val(many_larger_hidden_layers, NORMALIZED_INPUTS, TARGETS)\n",
    "print(\"Network testing complete\\n\\nMoving onto testing networks with both large hidden layers, and many hidden layers\")\n",
    "\n",
    "large_to_small_hidden_accuracy = five_fold_cross_val(large_to_small_hidden, NORMALIZED_INPUTS, TARGETS)\n",
    "small_to_large_hidden_accuracy = five_fold_cross_val(small_to_large_hidden, NORMALIZED_INPUTS, TARGETS)\n",
    "print(\"Network testing complete\\n\\nResults:\")\n",
    "\n",
    "print(f\"In between results:\\n\\tnodes = 10: {in_between_structure_1_accruacy * 100}%\\n\\tnodes = 7: {in_between_structure_2_accruacy * 100}%\\n\\tnodes = 4: {in_between_structure_3_accruacy * 100}%\")\n",
    "print(f\"Large hidden number of nodes in hidden layer results:\\n\\tnodes = 50: {large_hidden_layer_accuracy * 100}%\\n\\tnodes = 500: {extra_large_hidden_layer_accuracy * 100}%\\n\\tnodes = 1000: {extremely_large_hidden_layer_accuracy * 100}%\")\n",
    "print(f\"Many hidden layers results:\\n\\tlayers = 5, nodes = 4: {many_smaller_hidden_layers_accuracy * 100}%\\n\\tlayers = 7, nodes = 7: {many_larger_hidden_layers_accuracy * 100}%\")\n",
    "print(f\"Multiple hidden layers and many nodes results:\\n\\tlayer_1 = 50 nodes, layer_2 = 25 nodes, layer_3 = 7 nodes: {large_to_small_hidden_accuracy * 100}%\\n\\tlayer_1 = 7 nodes, layer_2 = 25 nodes, layer_3 = 50 nodes: {small_to_large_hidden_accuracy * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DwDJ3S04D6p"
   },
   "source": [
    "Given these results, the networks with the highest accuracy were networks with many nodes in a single hidden layer. <br><br>\n",
    "One final test is going to be network and algorithm effieciency. I am going to compare the speed of the 50 node architecture to the speed of the 1000 node architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_OsCV2Vctuk",
    "outputId": "27f28e10-080b-44a7-964e-5b0bc2a6424e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network with 50 hidden nodes took 21.94187021255493 seconds to train\n",
      "The network with 1000 hidden nodes took 38.62543773651123 seconds to train\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "five_fold_cross_val(large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "end = time.time()\n",
    "smaller_effieciency = end - start\n",
    "\n",
    "start = time.time()\n",
    "five_fold_cross_val(extremely_large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "end = time.time()\n",
    "larger_efficiency = end - start\n",
    "\n",
    "print(f\"The network with 50 hidden nodes took {smaller_effieciency} seconds to train\")\n",
    "print(f\"The network with 1000 hidden nodes took {larger_efficiency} seconds to train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnE-e8Hc75UZ"
   },
   "source": [
    "# Conclusion\n",
    "In conclusion, the multi layer perceptron was able to significantly improve the accuracy of predictions on this data. The baseline predictor had an average accuracy around 30%, and the basic multi layer perceptron predictor had an accuracy above 98%. After testing and optimization of the network architecture, it became apparent that a single hidden layer with many nodes was the network architecture that gave the greatest accuracy. Then after testing the training efficiency, it took about 3 times longer to train the network with 1000 nodes than the network with 50 nodes. Therefore the architecture with one hidden layer and 50 nodes in the layer is the best predictor I was able to find. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKaZgHwmAVuX",
    "outputId": "c343c365-b5b8-468e-e221-830ab20b58d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy using a network architecture with 50 nodes in the hidden layer is 98.31746031746033%\n"
     ]
    }
   ],
   "source": [
    "final_accuracy = five_fold_cross_val(large_hidden_layer, NORMALIZED_INPUTS, TARGETS)\n",
    "print(f\"Final accuracy using a network architecture with 50 nodes in the hidden layer is {final_accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "B455Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
